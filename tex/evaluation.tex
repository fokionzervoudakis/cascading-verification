\section{Evaluation}
\label{sec:Evaluation}

\noindent We assert that by enhancing the abstraction level of model and property specifications, cascading verification also enhances the effectiveness of probabilistic model checking. To validate this assertion, we will demonstrate that, as an implementation of cascading verification for the UAV domain, the prototype presented in this paper benefits mission developers by simplifying the verification of UAV mission plans, and by augmenting PRISM's verification capabilities. Ultimately, we aim to show that our prototype benefits mission developers by improving the correctness of UAV mission specifications. We will also evaluate the portability of cascading verification, i.e., the usability of our method in the context of different application domains.

\subsection{Abstraction}

\noindent Because it was unfeasible to involve practitioners in the evaluation of our prototype's utility, we opted instead for a metrics-based analysis of~58 mission plans. These plans were based on real-world mission scenarios developed independently by DARPA and DRDC~\cite{DARPA,Youngson_2004}. We evaluated our approach by comparing the \emph{lines of code} (LOC) and numbers of lexical tokens required to specify missions in YAML against the LOC and tokens in the combined DTMC and PCTL code synthesized by the CVC\@. On average, our prototype synthesizes PRISM code that is 3.127 and 4.490 times greater than the size of YAML input with regard to LOC and tokens, respectively. (The standard deviations were 52.4\% and 95.4\%, respectively.) These results, which are presented succinctly in the interest of space, provide preliminary evidence of non-trivial reduction in the effort required to produce mission models and properties.

We observe that tactical missions generate more LOC and tokens than \emph{standalone mission plans}, which are mission plans not associated with the more specialized tactical subdomain. Specifically, tactical mission plans generate PRISM code that is on average 3.933 and 5.992 times greater than the size of YAML input with regard to LOC and tokens, respectively. (The standard deviations were 24.0\% and 59.2\%, respectively.) Because the effort required to synthesize PRISM code is proportional to the effort required to synthesize the LOC and tokens that constitute the code, tactical mission plans result in added value for mission developers. This observation suggests that, with respect to tactical missions, the utility of our prototype is proportional to the threat level associated with any given mission plan. More broadly, increased LOC and token output suggests that the utility of cascading verification may be proportional to the amount of automated reasoning required to synthesize pertinent artifacts, a conclusion that justifies our motivation to augment model checking with formalized domain knowledge.

\subsection{Effectiveness}

\noindent Because it cannot account for the intricate syntax of the PRISM language, a LOC- and token-based analysis offers limited insight into the inherent complexity of model and property specifications. We investigate complexity further by considering \emph{behavioral modeling errors} specific to the PRISM language that can be eliminated with the automated synthesis of PRISM artifacts (at least for the segment of the mission space that we have explored thus far). Behavioral modeling errors include variable declarations with incorrect values; incorrect or missing command actions; incorrect command probabilities; and incorrect command updates (PRISM language constructs were introduced in Section~\ref{sec:Behavioral_Modeling}). These errors are significant, perhaps more so than the errors uncovered during the model checking process, because they can mislead mission developers by causing PRISM to verify erroneous mission plans.

We also consider mission specification errors that are beyond the scope of PRISM's verification capabilities. These \emph{domain-specific errors} are detected by either Pellet or the SWI-Prolog compiler during the synthesis process. We have identified 28 domain-specific errors, across six error classes, that impact the correctness of UAV missions. In the context of the OWL language and CEMO:

\begin{itemize}

\item \emph{Disjoint class errors} occur when individuals are declared in system specifications to be instances of incompatible classes; for example, a hover action can also be a kinetic action, but not a sensor action.

\item \emph{Existential restriction errors} occur when individuals fail to participate in mandatory relationships, as specified by the OWL keyword \texttt{some}; for example, every asset must execute at least one kinetic action.

\item \emph{Data property value errors} occur when data property values declared in system specifications fall outside the ranges of corresponding data properties encoded in CEMO (data properties were described in Section~\ref{sec:Semantic_Modeling}); for example, the endurance of a Hummingbird is 1200 time-steps.

\item \emph{Data property domain and range errors} occur when data property domain and range types declared in system specifications are inconsistent with the domain and range types of the corresponding data properties encoded in CEMO; for example, CEMO specifies that every \texttt{Hummingbird} individual must be associated with a datatype property \texttt{hasCostValue} of type \texttt{int}.

\item \emph{Object property domain and range errors} occur when object property domain and range types declared in system specifications are inconsistent with the domain and range types of the corresponding object properties encoded in CEMO (object properties were described in Section~\ref{sec:Semantic_Modeling}); for example, CEMO specifies that the object property \texttt{hasAction} associates every member of class \texttt{Asset} (the domain of \texttt{hasAction}) with a member of class \texttt{KineticAction} (the range).

\item \emph{Threatened asset errors} occur when mission plans comprise a threatened asset that is not also a valid asset (threatened and valid assets were described in Section~\ref{sec:Method_Overview} and Section~\ref{sec:Behavioral_Modeling}, respectively).

\end{itemize}

Mission correctness can clearly be compromised by do\-main-specific and behavioral modeling errors, which occur during the design/implementation and verification phases, respectively, of the mission development process. Our prototype augments PRISM's effectiveness by preventing both of these error types.

\subsection{Probabilistic Verification}

\noindent Finally, we consider PRISM's ability to meaningfully verify UAV mission plans (or rather, we consider the utility of the DTMC and PCTL artifacts synthesized by our prototype). For this part of the evaluation, eighteen of the~58 mission plans described above were seeded with errors, including deadlock and non-reachable states, that violated desirable behavioral properties. One mission plan failed (i.e., contained errors that resulted in a 0.0 probability of success) because of an unacceptably low RAF value; nine mission plans failed because kinetic or sensor action workflow durations exceeded the endurances of the assets to which those workflows were assigned; and eight mission plans contained action workflow errors that resulted in deadlock. These errors were successfully identified by our prototype. While the correctness of some mission plans was absolute (with a 0.0 or 1.0 probability of success) several mission plans, including plans comprising threat area incursions, were associated with variable probabilities of success. For example, the probability of success for Mission~A is approximately 0.299 (as described in Section~\ref{sec:Synthesized_Models_and_Properties}).

\subsection{Discussion}

\noindent By automating the synthesis of PRISM artifacts, and by providing multiple stages of reasoning and analysis, our prototype enhances the abstraction level of model and property specifications, and the effectiveness of probabilistic model checking, respectively. This cascading approach to verification improves mission correctness to a degree that is evidently unattainable by the individual components that constitute the prototype.

In conjunction with the complexity of the UAV domain, and the real-world DARPA and DRDC mission scenarios underpinning this evaluation, the above results suggest that cascading verification can be ported to different application domains. The portability of our method is supported by the general purpose of its constituent technologies including OWL+SWRL, Prolog, and DTMC and PCTL\@. Presently, we cannot make the same argument for the \emph{connections} that link those technologies in the context of the method. But cascading verification is an extension of semantic model checking methods with identical or comparable constituent technologies (similarities to semantic model checking will be presented in Section~\ref{sec:Related_Work}). The successful application of these methods to the Web service domain further supports the portability of cascading verification.

We note that this evaluation is preliminary. Further work is required to determine the utility of our prototype in the context of a more sophisticated mission specification language and domain model; and the ability of cascading verification to support probabilistic model checking in the context of other non-trivial domains.
